import streamlit as st
import pandas as pd
import pickle
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
from utils.data_preprocessing import load_and_preprocess_data

# Utility: safe correlation
def safe_corrcoef(a: np.ndarray, b: np.ndarray) -> float:
    a = np.asarray(a)
    b = np.asarray(b)
    if a.size == 0 or b.size == 0:
        return np.nan
    if np.allclose(np.std(a), 0) or np.allclose(np.std(b), 0):
        return np.nan
    return float(np.corrcoef(a, b)[0, 1])

st.title("ðŸ“ˆ Model Analytics")

# Load models
@st.cache_resource
def load_models():
    log_reg = pickle.load(open("models/logistic_regression.pkl", "rb"))
    catboost = CatBoostClassifier()
    catboost.load_model("models/catboost_model.cbm")
    return log_reg, catboost

try:
    log_reg, catboost = load_models()
    
    # Load and prepare data
    df = load_and_preprocess_data()
    df = df.dropna()
    
    X = df.drop("SeriousDlqin2yrs", axis=1)
    y = df["SeriousDlqin2yrs"]
    
    # Load scaler for logistic regression
    scaler = pickle.load(open("models/scaler.pkl", "rb"))
    X_scaled = scaler.transform(X)
    
    # Calculate predictions
    log_pred_proba = log_reg.predict_proba(X_scaled)[:, 1]
    cat_pred_proba = catboost.predict_proba(X)[:, 1]
    
    log_pred = log_reg.predict(X_scaled)
    cat_pred = catboost.predict(X)
    
    # Calculate metrics
    log_auc = roc_auc_score(y, log_pred_proba)
    cat_auc = roc_auc_score(y, cat_pred_proba)
    
    # Display metrics
    st.write("### Model Performance Metrics")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Logistic Regression AUC", f"{log_auc:.3f}")
    with col2:
        st.metric("CatBoost AUC", f"{cat_auc:.3f}")
    
    # Model comparison
    st.write("### Model Comparison")
    comparison_data = {
        'Model': ['Logistic Regression', 'CatBoost'],
        'AUC Score': [log_auc, cat_auc]
    }
    comparison_df = pd.DataFrame(comparison_data)
    st.dataframe(comparison_df)
    
    # Create comparison chart
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(comparison_df['Model'], comparison_df['AUC Score'])
    ax.set_ylabel('AUC Score')
    ax.set_title('Model Performance Comparison')
    ax.set_ylim(0, 1)
    
    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom')
    
    st.pyplot(fig)
    
    # ROC Curves
    st.write("### ROC Curves")
    
    # Calculate ROC curves
    fpr_lr, tpr_lr, _ = roc_curve(y, log_pred_proba)
    fpr_cat, tpr_cat, _ = roc_curve(y, cat_pred_proba)
    
    # Create ROC curve with Plotly
    fig_roc = go.Figure()
    
    fig_roc.add_trace(go.Scatter(
        x=fpr_lr, y=tpr_lr,
        mode='lines',
        name=f'Logistic Regression (AUC: {log_auc:.3f})',
        line=dict(color='blue', width=3)
    ))
    
    fig_roc.add_trace(go.Scatter(
        x=fpr_cat, y=tpr_cat,
        mode='lines',
        name=f'CatBoost (AUC: {cat_auc:.3f})',
        line=dict(color='orange', width=3)
    ))
    
    fig_roc.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1],
        mode='lines',
        name='Random Classifier (AUC: 0.5)',
        line=dict(dash='dash', color='red')
    ))
    
    fig_roc.update_layout(
        title="ROC Curves Comparison",
        xaxis_title="False Positive Rate",
        yaxis_title="True Positive Rate",
        height=500,
        xaxis=dict(range=[0, 1]),
        yaxis=dict(range=[0, 1])
    )
    
    st.plotly_chart(fig_roc, use_container_width=True)
    
    # Prediction Distributions
    st.write("### Prediction Distributions")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Logistic Regression Predictions**")
        fig_lr_dist = px.histogram(
            x=log_pred_proba,
            nbins=50,
            title="LR Prediction Distribution",
            labels={'x': 'Risk Probability', 'y': 'Frequency'},
            color_discrete_sequence=['blue']
        )
        fig_lr_dist.add_vline(x=np.mean(log_pred_proba), line_dash="dash", line_color="red",
                             annotation_text=f"Mean: {np.mean(log_pred_proba):.3f}")
        fig_lr_dist.update_layout(height=400)
        st.plotly_chart(fig_lr_dist, use_container_width=True)
        
        # Statistics
        st.write(f"**Mean:** {np.mean(log_pred_proba):.3f}")
        st.write(f"**Std Dev:** {np.std(log_pred_proba):.3f}")
        st.write(f"**Min:** {np.min(log_pred_proba):.3f}")
        st.write(f"**Max:** {np.max(log_pred_proba):.3f}")
    
    with col2:
        st.write("**CatBoost Predictions**")
        fig_cat_dist = px.histogram(
            x=cat_pred_proba,
            nbins=50,
            title="CatBoost Prediction Distribution",
            labels={'x': 'Risk Probability', 'y': 'Frequency'},
            color_discrete_sequence=['orange']
        )
        fig_cat_dist.add_vline(x=np.mean(cat_pred_proba), line_dash="dash", line_color="red",
                              annotation_text=f"Mean: {np.mean(cat_pred_proba):.3f}")
        fig_cat_dist.update_layout(height=400)
        st.plotly_chart(fig_cat_dist, use_container_width=True)
        
        # Statistics
        st.write(f"**Mean:** {np.mean(cat_pred_proba):.3f}")
        st.write(f"**Std Dev:** {np.std(cat_pred_proba):.3f}")
        st.write(f"**Min:** {np.min(cat_pred_proba):.3f}")
        st.write(f"**Max:** {np.max(cat_pred_proba):.3f}")
    
    # Prediction Correlation
    st.write("### Prediction Correlation Analysis")
    correlation = safe_corrcoef(log_pred_proba, cat_pred_proba)
    st.write(f"**Correlation between model predictions:** {correlation:.3f}" if not np.isnan(correlation) else "Correlation could not be computed (insufficient variance)")
    
    # Scatter plot of predictions
    fig_scatter = px.scatter(
        x=log_pred_proba,
        y=cat_pred_proba,
        title="Model Predictions Correlation",
        labels={'x': 'Logistic Regression', 'y': 'CatBoost'},
        color_discrete_sequence=['purple']
    )
    
    # Add perfect agreement line
    fig_scatter.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1], mode='lines', name='Perfect Agreement',
        line=dict(dash='dash', color='red')
    ))
    
    fig_scatter.update_layout(height=500)
    st.plotly_chart(fig_scatter, use_container_width=True)
    
    # Feature Importance Analysis
    st.write("### Feature Importance Analysis")
    
    try:
        # Get feature names
        feature_names = X.columns.tolist()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Logistic Regression Coefficients**")
            # For logistic regression, use absolute coefficients
            lr_importance = np.abs(log_reg.coef_[0])
            lr_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': lr_importance
            }).sort_values('Importance', ascending=True)
            
            fig_lr_imp = px.bar(
                lr_importance_df,
                x='Importance',
                y='Feature',
                orientation='h',
                title="LR Feature Importance",
                color='Importance',
                color_continuous_scale='viridis'
            )
            fig_lr_imp.update_layout(height=500)
            st.plotly_chart(fig_lr_imp, use_container_width=True)
        
        with col2:
            st.write("**CatBoost Feature Importance**")
            # For CatBoost, use feature importance
            cat_importance = catboost.feature_importances_
            cat_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': cat_importance
            }).sort_values('Importance', ascending=True)
            
            fig_cat_imp = px.bar(
                cat_importance_df,
                x='Importance',
                y='Feature',
                orientation='h',
                title="CatBoost Feature Importance",
                color='Importance',
                color_continuous_scale='plasma'
            )
            fig_cat_imp.update_layout(height=500)
            st.plotly_chart(fig_cat_imp, use_container_width=True)
        
        # Feature importance comparison (use long-form to avoid type issues)
        st.write("**Feature Importance Comparison**")
        comparison_imp_df = pd.DataFrame({
            'Feature': feature_names,
            'Logistic_Regression': lr_importance.astype(float),
            'CatBoost': np.asarray(cat_importance, dtype=float)
        })
        comparison_imp_long = comparison_imp_df.melt(
            id_vars=['Feature'],
            value_vars=['Logistic_Regression', 'CatBoost'],
            var_name='Model',
            value_name='Importance'
        )
        comparison_imp_long['Importance'] = pd.to_numeric(comparison_imp_long['Importance'], errors='coerce')
        fig_comp_imp = px.bar(
            comparison_imp_long,
            x='Feature',
            y='Importance',
            color='Model',
            title="Feature Importance Comparison",
            barmode='group'
        )
        fig_comp_imp.update_layout(height=500, xaxis_tickangle=-45)
        st.plotly_chart(fig_comp_imp, use_container_width=True)
        
    except Exception as e:
        st.warning(f"Could not generate feature importance plots: {e}")
    
    # Confusion matrices
    st.write("### Confusion Matrices")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Logistic Regression**")
        cm_lr = confusion_matrix(y, log_pred)
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=ax)
        ax.set_title('Logistic Regression Confusion Matrix')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        st.pyplot(fig)
    
    with col2:
        st.write("**CatBoost**")
        cm_cat = confusion_matrix(y, cat_pred)
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm_cat, annot=True, fmt='d', cmap='Greens', ax=ax)
        ax.set_title('CatBoost Confusion Matrix')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        st.pyplot(fig)
    
    # Classification reports
    st.write("### Classification Reports")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Logistic Regression**")
        st.text(classification_report(y, log_pred))
    
    with col2:
        st.write("**CatBoost**")
        st.text(classification_report(y, cat_pred))
    
    # Risk Score Analysis
    st.write("### Risk Score Analysis")
    
    # Create risk categories
    risk_categories = {
        'Very Low': (0, 0.2),
        'Low': (0.2, 0.4),
        'Medium': (0.4, 0.6),
        'High': (0.6, 0.8),
        'Very High': (0.8, 1.0)
    }
    
    # Analyze risk distribution for both models
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Logistic Regression Risk Distribution**")
        lr_risk_dist = []
        for category, (min_val, max_val) in risk_categories.items():
            count = np.sum((log_pred_proba >= min_val) & (log_pred_proba < max_val))
            lr_risk_dist.append(count)
        
        fig_lr_risk = px.pie(
            values=lr_risk_dist,
            names=list(risk_categories.keys()),
            title="LR Risk Distribution",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        fig_lr_risk.update_layout(height=400)
        st.plotly_chart(fig_lr_risk, use_container_width=True)
    
    with col2:
        st.write("**CatBoost Risk Distribution**")
        cat_risk_dist = []
        for category, (min_val, max_val) in risk_categories.items():
            count = np.sum((cat_pred_proba >= min_val) & (cat_pred_proba < max_val))
            cat_risk_dist.append(count)
        
        fig_cat_risk = px.pie(
            values=cat_risk_dist,
            names=list(risk_categories.keys()),
            title="CatBoost Risk Distribution",
            color_discrete_sequence=px.colors.qualitative.Set2
        )
        fig_cat_risk.update_layout(height=400)
        st.plotly_chart(fig_cat_risk, use_container_width=True)
    
    # PDF Report Generation Section
    st.write("---")
    st.write("### ðŸ“„ Generate Model Analysis Report")
    st.write("Create a comprehensive PDF report of the model analysis and performance metrics.")
    
    if st.button("ðŸ“Š Generate Model Analysis Report", type="primary"):
        try:
            from utils.pdf_generator import generate_pdf_report
            import os
            from datetime import datetime
            
            with st.spinner("Generating comprehensive model analysis report..."):
                # Create a comprehensive model analysis report
                # We'll use sample data since this is for demonstration
                sample_input_data = {
                    'Model_Analysis_Date': datetime.now().strftime("%Y-%m-%d"),
                    'Total_Samples': len(df),
                    'Positive_Cases': int(y.sum()),
                    'Negative_Cases': int(len(y) - y.sum()),
                    'Logistic_Regression_AUC': f"{log_auc:.3f}",
                    'CatBoost_AUC': f"{cat_auc:.3f}",
                    'Best_Model': 'CatBoost' if cat_auc > log_auc else 'Logistic Regression'
                }
                
                sample_risk_factors = {
                    'Model Performance': max(log_auc, cat_auc),
                    'Data Quality': 0.85,  # Sample metric
                    'Feature Importance': 0.78,  # Sample metric
                    'Prediction Stability': 0.82,  # Sample metric
                    'Overall Model Health': 0.80  # Sample metric
                }
                
                # Generate model analysis report
                pdf_path = generate_pdf_report(
                    name="Model Analysis",
                    prediction=f"{max(log_auc, cat_auc):.3f}",
                    input_data=sample_input_data,
                    risk_factors=sample_risk_factors
                )
            
            if pdf_path:
                st.success("âœ… Model Analysis Report generated successfully!")
                
                # Read and provide download button
                with open(pdf_path, "rb") as pdf_file:
                    pdf_bytes = pdf_file.read()
                
                st.download_button(
                    label="ðŸ“¥ Download Model Analysis Report (PDF)",
                    data=pdf_bytes,
                    file_name=f"model_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                    mime="application/pdf",
                    help="Download the comprehensive model analysis report",
                    use_container_width=True
                )
                
                st.info("**Report Includes:**")
                st.write("â€¢ Model performance metrics and comparison")
                st.write("â€¢ ROC curves and AUC scores")
                st.write("â€¢ Feature importance analysis")
                st.write("â€¢ Risk assessment and recommendations")
                st.write("â€¢ Data quality metrics")
                st.write("â€¢ Model health indicators")
                
            else:
                st.error("Failed to generate model analysis report")
                
        except Exception as e:
            st.error(f"Error generating model analysis report: {str(e)}")
            st.info("Please ensure all required dependencies are installed")

except Exception as e:
    st.error(f"Error loading models: {str(e)}")
    st.info("Please ensure models are trained first by running: python utils/model_training.py") 